% Proposal for Convex Latent Variable
% Author:
%   Jimmy Lin
%   Ian Yen

\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb,amsmath,amsfonts,latexsym,mathtext}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Project Proposal: Convex Latent Variable}

%{{{ Authors
\author{
Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}
%}}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%{{{ Macros
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\argmax}[1]{\underset{#1}{\text{argmax} }}

\newcommand{\indicator}[1]{\mathbf{1}_{#1}}
\newcommand{\LTwoNorm}[1]{||#1||^{2}}
\newcommand{\sumn}{\sum_{n}}
\newcommand{\sumk}{\sum_{k}}

\newcommand{\wnk}{w_{nk}}
\newcommand{\wn}{\mathbf{w}_n}
\newcommand{\wone}{\mathbf{w}_1}
\newcommand{\wtwo}{\mathbf{w}_2}
\newcommand{\wthree}{\mathbf{w}_3}
\newcommand{\wfour}{\mathbf{w}_4}
\newcommand{\w}{\mathbf{w}}
\newcommand{\wbar}{\overline{\w}}
\newcommand{\wopt}{\mathbf{w^{*}}}
\newcommand{\wnbyn}{\mathbf{w}_{n\times n}}

\newcommand{\x}[1]{\mathbf{x}_{#1}}
\newcommand{\xn}{\mathbf{x}_n}
\newcommand{\muk}{\boldsymbol{\mu}_k}
\newcommand{\maxn}{ \underset{n}{\text{max}} }
\newcommand{\maxk}{ \underset{k}{\text{max}} }

\newcommand{\yone}{\mathbf{y}_1}
\newcommand{\ytwo}{\mathbf{y}_2}
\newcommand{\ythree}{\mathbf{y}_3}
\newcommand{\yfour}{\mathbf{y}_4}
\newcommand{\z}{\mathbf{z}}
\newcommand{\quadraterm}[1]{\frac{\rho}{2}\LTwoNorm{\w_{#1}-\z}}
\newcommand{\dualterm}[1]{\mathbf{y}_{#1}^{T}(\w_{#1}-\z)}

\newcommand{\minimize}[1]{ \underset{#1}{\text{Minimize}} }
\newcommand{\maximize}[1]{ \underset{#1}{\text{Maximize}} }
\newcommand{\subjectto}{ \text{subject to} }
\newcommand{\hs}{\hspace*{0.6cm}}

%}}}

%\nipsfinalcopy % Uncomment for camera-ready version

%%% TODO list:
%% 1. reason of applying group-lasso (one or zero)
%% 2.

\begin{document}

\maketitle

\begin{abstract}
    Add abstract here..
\end{abstract}

\section{Problem Formulation}
\subsection{Motivation}
Given a set of data entities $\x{1}, ..., \xn, ..., \x{N} $, we wish to
automatically cluster these entities without specifying the number of cluster
centroids. One possible approach is to regard these input entities as
potential cluster candidates and evaluate the "belonging matrix" $\wnbyn$, consisting
of latent variables $\wnk$ indicating the extent of one entity $\xn$ explained
by one potential cluster centroid candidate $\muk$. In this project, we
propose a method to compute the sparse latent variable with faster
convergence by decomposing the objective function derived by ADMM into two
separate optimization task.


\subsection{Overall Optimization Goal}
Formally, our goal is to achieve clustering with group-lasso regularization.
Hence, we formulate the target problem in terms of notations introduced above
as follows:

 \begin{align}
  \minimize{\w}
  \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
        + \lambda \sumk \maxn | \wnk |  \\ % group-lasso
  \subjectto
  \hs & \forall n,\ \sumk \wnk \leq 1 \\
  \hs & \forall n,\ k,\ \wnk \geq 0
 \end{align}

The limiting conditions listed above are jointly called simplex constraint.
Every entity must be assigned to some centroid with prob 1.
Every entity must have non-negative belonging index to each centroid candidate.

Note that the group-lasso can either be one or zero in the context that we
pick up the most promising centroid candidates from provided entities.

\subsection{ADMM}

We can rewrite the initial optimization goal as the following primal problem:

 \begin{align}
  \minimize{\wone, \wtwo, \z}
  \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
    + \quadraterm{1} \nonumber \\
    & \hs + \lambda \sumk \maxn | \wnk |
    + \quadraterm{2}
        \\ % group-lasso
  \subjectto
  \hs & \w_{1} = \z,\ \w_{2} = \z
 \end{align}

 Note that we separate $\w$ into two variables, $\wone$ and $\wtwo$
 respectively. This is to prepare it for latter optimization decomposition.

By dual decomposition, the problem can be further transformed as follows:

 \begin{align}
     \minimize{\wone, \wtwo, \z} \hs  \maximize{\yone, \ytwo}
   \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
   + \dualterm{1} + \quadraterm{1} \nonumber \\
    & \hs + \lambda \sumk \maxn | \wnk |  % group-lasso
   + \dualterm{2} + \quadraterm{2}
 \end{align}

 We continue by incorporting duality and derive the following optimization
 task:

  \begin{align}
     \maximize{\yone, \ytwo} \hs  \minimize{\wone, \wtwo, \z}
   \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
   + \dualterm{1} + \quadraterm{1} \nonumber \\
    & \hs + \lambda \sumk \maxn | \wnk |  % group-lasso
   + \dualterm{2} + \quadraterm{2}
 \end{align}

 \subsection{Optimization Decomposition} \label{separate}
\newcommand{\mutw}[1]{\mathbf{y}_{#1}^{T} \w_{#1}}

The first optimization task:
  \begin{align}
   \minimize{\wone}
   \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
   + \mutw{1} + \quadraterm{1} \\
   \subjectto
   \hs & \forall n,\ \sumk \wnk \leq 1 \\
   \hs & \forall n,\ k,\ \wnk \geq 0
 \end{align}
 This subproblem can be solved by Frank-Wolf Algorithm. 

 The second optimization task:
 \begin{align} \label{second}
   \minimize{\wtwo}
   \hs & \lambda \sumk \maxn | \wnk |  % group-lasso
   + \mutw{2} + \quadraterm{2} 
 \end{align}

 The closed-form solution of second subproblem can be found through Blockwise
 Coordinate Descent Procedures (Han Liu.). 

\subsection{Overall Solution Procedure}
Repeat

1. Resolve $\wone$ and $\wtwo$ in separate subproblem formulated in section
\ref{separate}.

2. Update the combination indicator $\z$ by $\z = \frac{1}{2} (\wone + \wtwo)$

3. Refine the multiplier $\yone$ and $\ytwo$ with gradient update rule: 
\begin{align}
\yone = \yone - \alpha \cdot (\wone - \z) \\
\ytwo = \ytwo - \alpha \cdot (\wtwo - \z)
\end{align}

Until Convergence.

\subsection{Resolve first subproblem: Frank-Wolf Algorithm} 

\subsection{Resolve second subproblem: Blockwise Coordinate Descent Procedures} 
According to Blockwise Coordinate Descent Procedures (Han Liu.), the way to
resolve the \eqref{second} optimization problem is first to seek solution $\wbar$ without
group-lasso \eqref{noglasso} as shown below. 

\begin{align} \label{noglasso}
   \minimize{\wtwo}
   \hs & \mutw{2} + \quadraterm{2} 
 \end{align}

\newcommand{\ms}{m^{*}}
Then ultimately find the closed-form solution in terms of $\wbar$. In
practice, we resolve it column by column. The formula for closed-form
solution $\wopt$ is given as follows:

\begin{align}
    \wopt = ...
\end{align}

where $\ms = \argmax{m} \frac{1}{m} \big(\sum_{i'}^{m} |\wopt^{(k_{i'})}| -
\lambda \big)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Application on Topic Model}
\newcommand{\lambdag}{\lambda_g}
\newcommand{\lambdal}{\lambda_l}
\newcommand{\lambdab}{\lambda_b}


\subsection{Problem Formulation}
In this topic model, there are four main semantics supposed to be captured.

First key point is that all words in any document should be related to one
topic. This can be regarded as resources (words) given out to all receivers
(topics). And intuitively, this term support the value of components within
$w$. Formally, this is represented by equality constraints.

Secondly, too many hidden topics are supposed to be penalized. Intuitively,
this objective aimed at reducing total number of topics. Formally, one
regularization term is used as follows: 
\begin{align}
   \lambdag \sumk \maxn |\wnk| 
\end{align}

Thirdly, too many topics involved by each document should be penalized.
However, the situation of no topics to explain one document is
non-existent since we enforce one constraint that all words must be related to
certain topics with total explanation.  Hence, this objective intuitively
aimed at reducing topic distribution within one document. To capture this
semanticcs, we use another regularizer:
\begin{align}
\lambdal \sumn \maxk |\wnk|
\end{align}

Fourthly, too many vocabularies involved by each topic ought to be penalized.
This would increase the number of hidden topics within this model. It can be
intuitively seen as an support of topics number or cause of matrix extension
of $w$. One extra regularizer is employed:
\begin{align}
\lambdab \sumk \sum_v \max |\wnk^{(v)}|
\end{align}
To acquire small penalty from the last regularization, all documents that are
related to certain topic should be attributed to a narrow set of vocabularies. 

\subsection{Objectives}
\begin{align}
   \minimize{w}
   \hs & \lambdag \sumk \maxn |\wnk| 
        + \lambdal \sumn \maxk |\wnk| 
        + \lambdab \sumk \sum_v \max |\wnk^{(v)}| \\
   \subjectto
   \hs & \forall n,\ k,\ 0 \leq  \wnk \leq 1 \\
    \hs & \forall n,\ \sumk \wnk = 1
\end{align}

 \subsection{Optimization Decomposition} \label{nseparate}

The first optimization task:
  \begin{align}
   \minimize{\wone}
   \hs & r \cdot \big| 1 - \sumk \wnk \big|   % dummy loss
   + \mutw{1} + \quadraterm{1} \\
   \subjectto
   \hs & \forall n,\ \sumk \wnk \leq 1 \\
   \hs & \forall n,\ k,\ \wnk \geq 0
 \end{align}
 where $r$ is a arbitrary value to enforce the resource to be fully
 distributed. Note that this subproblem can be solved by Frank-Wolf Algorithm. 

 The second optimization task:
 \begin{align} \label{second}
   \minimize{\wtwo}
   \hs & \lambdag \sumk \maxn |\wnk|  % group-lasso
   + \mutw{2} + \quadraterm{2} 
 \end{align}

  The third optimization task:
 \begin{align} \label{second}
   \minimize{\wthree}
   \hs & \lambdal \sumn \maxk |\wnk|  % group-lasso
   + \mutw{3} + \quadraterm{3} 
 \end{align}

   The fourth optimization task:
 \begin{align} \label{second}
   \minimize{\wfour}
   \hs &  \lambdab \sumk \sum_v \max |\wnk^{(v)}|  % group-lasso
   + \mutw{4} + \quadraterm{4} 
 \end{align}

 The closed-form solution of 2-4 subproblems can be found through Blockwise
 Coordinate Descent Procedures (Han Liu.). 

\subsection{Overall Solution Procedure}
Repeat

1. Resolve $\wone$, $\wtwo$, $\wthree$, $\wfour$ in separate subproblem
formulated in section \ref{nseparate}.

2. Update the combination indicator $\z$ by $\z = \frac{1}{4} (\wone + \wtwo +
\wthree + \wfour)$

3. Refine the multiplier $\yone$ and $\ytwo$ with gradient update rule: 
\begin{align}
\yone = \yone - \alpha \cdot (\wone - \z) \\
\ytwo = \ytwo - \alpha \cdot (\wtwo - \z) \\ 
\ythree = \ythree - \alpha \cdot (\wthree - \z) \\
\yfour = \yfour - \alpha \cdot (\wfour - \z)
\end{align}

Until Convergence.

\subsection{Implementation Details}
To handle large scale data and manage extensibility, we implement a new data
structure - extensible sparse matrice. 

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Acknowledgments}

\subsubsection*{References}
\small{

}

\end{document}
